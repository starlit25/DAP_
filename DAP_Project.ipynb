{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DAP Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNCV1XIRZwRr7AIkFkAHceb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/starlit25/DAP_/blob/main/DAP_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGCCKROKR3pu"
      },
      "source": [
        "[진행단계]\n",
        "1. 11월 셋째주~12월 첫째주: 데이터 수집\n",
        "=> 실습 참고용으로 업로드 되어있는 영상의 웹 크롤링 부분 youtube 영상시청(완료)\n",
        "=> 몇몇개의 페이지 내 웹 스크래핑(완료)\n",
        "=> 모으고자 하는 데이터가 csv나 html형태가 아닌 파일 형태로 되어있는 입시 데이터 파일임으로, 파일 스크래핑을 해야해서 새롭게 다시 공부(진행중)\n",
        "=> 유튜브 외 다른 사람들의 코드 분석하며 공부(진행중)\n",
        "2. 12월 둘째주: 데이터 분석&코드 리뷰\n",
        "3. 12월 셋째주: 대쉬보드 그리기 및 프리젠테이션 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "DexaptmYRHw3",
        "outputId": "a7710d0a-76ca-47df-fdec-a7f1b92ad4e1"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import re\n",
        "from urllib import request\n",
        "from urllib.error import HTTPError\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "​\n",
        "overlap=[]\n",
        "url = 'https://www.kangwon.ac.kr/admission/selectBbsNttList.do?bbsNo=376&key=2146&'  #반복되는 곳 사이트 몸통\n",
        "site = 'https://www.kangwon.ac.kr/admission/'   #가져올 사이트 앞부분\n",
        "rec = \"/selectBbsNttList.do?bbsNo=376&key=2146&\" #반복되는 부분 뒷부분\n",
        "dl = \"/selectBbsNttList.do?bbsNo=376&key=2146&\"   #클릭해서 다운 받는 부분\n",
        "​\n",
        "def get_download(url, fname, directory):\n",
        "   try:\n",
        "       os.chdir(directory)\n",
        "       request.urlretrieve(url, fname)\n",
        "       print('download complete\\n')\n",
        "   except HTTPError as e:\n",
        "       print('error')\n",
        "       return\n",
        "​\n",
        "def downSearch(getDLATag):\n",
        "   for getDLLink in getDLATag:\n",
        "       try:\n",
        "           if dl in getDLLink.get('href'):\n",
        "               print(\"다운로드 링크 : {}\".format(site) + getDLLink.get('href'))\n",
        "               accessDLUrl = site + getDLLink.get('href')\n",
        "               fileOriginalNM = re.sub('<.+?>', '', str(getDLLink), 0).strip().replace('_', ' ')\n",
        "               fileNM = \"수시\" + fileOriginalNM\n",
        "               path = \"D:\\\\KAMIS\\\\\"\n",
        "               if os.path.isfile(path + fileNM):\n",
        "                   print(\"download fail\\n\")\n",
        "               else:\n",
        "                   get_download(accessDLUrl, fileNM, path)\n",
        "       except:pass\n",
        "​\n",
        "def Search(getA):\n",
        "   for getLink in getA:\n",
        "       data = getLink.get('href')\n",
        "       try:\n",
        "           if rec in getLink.get(\"href\"):\n",
        "               if len(data) >= 100 and data not in overlap:\n",
        "                   overlap.append(data)\n",
        "                   accessUrl = site + getLink.get(\"href\")\n",
        "                   r = requests.get(accessUrl)\n",
        "                   soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "                   getDLATag = soup.find_all(\"a\")\n",
        "                   downSearch(getDLATag)\n",
        "​\n",
        "               elif len(data) >= 85 and data not in overlap:\n",
        "                   overlap.append(data)\n",
        "                   accessUrl = site + getLink.get(\"href\")\n",
        "                   r = requests.get(accessUrl)\n",
        "                   soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "                   getDLATag = soup.find_all(\"a\")\n",
        "                   Search(getDLATag)\n",
        "                   # Search(getDLATag,depth+1)\n",
        "       except:pass\n",
        "​\n",
        "# request 모듈을 사용하여 웹 페이지의 내용을 가져온다\n",
        "r = requests.get(url)\n",
        "print(\"KAMIS 자료실 요청 : \", r)\n",
        "​\n",
        "# beautiful soup 초기화\n",
        "soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "# 태그로 찾기 (모든 항목)\n",
        "getA = soup.find_all(\"a\")\n",
        "Search(getA)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-6fbfa7e48d95>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    ​\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9iPbcWdRu5P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}